{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning attention block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that if I wanted to note down code and math, then the best place to do it is in a notebook.\n",
    "\n",
    "In this one, we will write down all the details of the self attention block. This is so pervasive now that I think it is worth the effort to understand it carefully.\n",
    "\n",
    "I have written down notes on the transformer before, though I stopped at the math level and didn't get to the code level. Today the goal is to do everything fully, and write down explanations of every line to make it ultra easy to recover next time.\n",
    "\n",
    "## Conceptual recap\n",
    "Input:\n",
    "- a window of length `w` of token embeddings of dim `n_embd_1`.\n",
    "- a batch size `B` of independent queries\n",
    "- a total of `N` embeddings in the dictionary. These embeddings are fixed an not trained. For instance for language models, we will be feeding in a window of `N` words at a time, and these words are converted to word embeddings in dim `n_embd_1`.\n",
    "    - I don't actually think we need this constraint that the number of embeddings be finite, because we can always translate to the query, key and value vectors using a linear layer. I will explain this later\n",
    "- So the final shape of the input will be `(B, w, n_embd_1)`\n",
    "\n",
    "Parameters:\n",
    "- For each embedding vector, we have a query vector $q_i$ of dim `n_embd_2`, key vector $k_i$ of dim `n_embd_2`, and a value vector $v_i$, of dim `n_embd_3`\n",
    "    - Note: the embedding dimensions could all be the same or different, but the important point is that $q_i$ and $k_i$ have the same simension, because we would want to form $\\langle q_i, k_j\\rangle$ between different embedding vectors.\n",
    "- The above parameters is only for a single attention head. For multi-head attentions, we would have `nh` different Q/K/V vectors for each embedding.\n",
    "\n",
    "The idea for a single attention head is as follows: given `w` input words, for each word in the end we want to generate another output vector of the same dim `n_embd_1` which depends on the other words in this window. The output will be a linear combination of the value vectors $v_i$s of the words (or rather a projection of the linear combination of the value vectors, if `n_embd_3` does not equal `n_embd_1`).\n",
    "\n",
    "How do we get the weights of the linear combination? For word $i$, the weights will be the softmax of $\\frac{1}{\\text{d2}}\\langle q_i, k_j\\rangle$ (d2 is `n_embd_2`), where $j$ ranges over the other `w` words in the window (idea being: you use the query vector $q_i$ to query against the keys of all the other words).\n",
    "\n",
    "And that's it. Then you repeat the process above for multi-head attention and concatenate and project into a final output.\n",
    "\n",
    "Final note: for text data, when we are doing next token generation, often we don't want early words to depend on the later words. So when getting the weights for word $i$, we might restrict it to using a linear combination of only $v_j$ for $j \\le i$. For this we need to do some `torch.tril` business that we'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "B = 8\n",
    "w = 16\n",
    "n_embd_1 = 16\n",
    "n_embd_2 = 32\n",
    "n_embd_3 = 32  # setting equal n_embd_2 for convenience\n",
    "nh = 8\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # attention block\n",
    "        # nn.Linear takes in shape (*, in_features) and outputs shape (*, out_features)\n",
    "        # for us, input shape will be (B, w, n_embd_1)\n",
    "        # c_attn contains all the weights for Q, K, V for nh heads\n",
    "        self.c_attn = nn.Linear(n_embd_1, 3 * nh * n_embd_2)\n",
    "\n",
    "        # the output from all the heads will be nh * n_embd_2\n",
    "        # need to project that down to n_embd_1\n",
    "        self.c_proj = nn.Linear(nh * n_embd_2, n_embd_1)\n",
    "        \n",
    "        # some regularization for the attention layer and output layer\n",
    "        self.attn_drop = nn.Dropout(0.1)\n",
    "        self.out_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        # Optional: a mask to ensure that token i only gets weights from j <= i\n",
    "        # not a trainable param, so use register buffer\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(w, w)).view(1, 1, w, w))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape (B, w, n_embd_1)\n",
    "        # support inference on window sizes smaller than w\n",
    "        # so all the operations are done with windon size T instead of the max w\n",
    "        T = x.size(1)\n",
    "\n",
    "        # split into q, k, v that each contains nh heads\n",
    "        # Tensor.chunk splits the tensor into specified number of chunks\n",
    "        # could also use Tensor.split(nh * n_embd_2, dim=-1), where we specify the size of each chunk\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, T, nh, n_embd_2).transpose(1, 2)  # (B, nh, T, n_embd_2)\n",
    "        k = k.view(B, T, nh, n_embd_2).transpose(1, 2)\n",
    "        v = v.view(B, T, nh, n_embd_2).transpose(1, 2)\n",
    "        \n",
    "        # attention weights\n",
    "        # @ multiplication only multiplies the last two dimensions\n",
    "        # attn_weights has shape (B, nh, T, T)\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * (1.0 / (n_embd_2 ** 0.5))\n",
    "        \n",
    "        # Optional: apply mask\n",
    "        # need to restrict mask to the first T to fit the input size\n",
    "        attn_weights = attn_weights.masked_fill(self.mask[:, :, :T, :T], float('-inf'))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_drop(attn_weights)\n",
    "        \n",
    "        # get output\n",
    "        output = attn_weights @ v\n",
    "        # right now the shape of output would be (B, nh, T, n_embd_2)\n",
    "        # want to concat all heads together into something of dim (B, T, nh * n_embd_2) and then apply projection\n",
    "        # this code is a bit suuble:\n",
    "        # first we transpose the output to (B, T, nh, n_embd_2)\n",
    "        # but transposing only changes the indexing and doesn't change the memory location\n",
    "        # so we need to call .contiguous to make sure the memory is laid out in the right order for .view\n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        \n",
    "        output = self.out_drop(self.c_proj(output))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
